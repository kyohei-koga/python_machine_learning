{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8.3 更に大規模なデータの処理：オンラインアルゴリズムとアウトオブコア学習\n",
    "\n",
    "- 多くの現実のアプリケーションでは、コンピュータのメモリに収まらないほど大規模なデータセットを処理することも珍しくない。\n",
    "\n",
    "- 誰もがスーパーコンピュータを利用できるわけではないため、ここではそうした大規模なデータセットの処理を可能にするアウトオブコア学習という手法を適用する。\n",
    "\n",
    "- ここでは、scikit-learnのSGDClassifierクラスのpartial_fitメソッドを使用することで、ローカルドライブからドキュメントを直接ストリーミングし、ドキュメントの小さなミニバッチを使って、ロジスティック回帰モデルをトレーニングする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>','',text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',text.lower())\n",
    "    text = re.sub('[\\W]+',' ',text.lower()) + ' '.join(emoticons).replace('-','')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ジェネレータ関数stream_docsを定義する。この関数は、ドキュメントを１つずつ読み込んで返す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r') as csv:\n",
    "        next(csv) #ヘッダーを読み飛ばす\n",
    "        for line in csv:\n",
    "            text, label = line[:-3],int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"I went to the cinema to watch a preview of this film without knowing anything about it. Recognizing Jennifer Lynch\\'s name and seeing the 18 certificate I realised it might be disturbing. In actuality I found the film a farce. I found myself giggling in disbelief through parts of it. The acting is atrocious- Bill Pullman and his ridiculous twitching face. I do almost pity the actors though as the script offers them no chance of any believable character interaction. After some shocking incident, (there is plenty to \"\"try\"\" and shock the viewer in this film), 2 characters are seen sharing a beer and talking about the weather. Everything was overstated, or thought it was being clever when really it was obvious! The performance from the little girl character named Stephanie was the best thing about the film. Quiet and intense. I really could not recommend this film to anyone. Its violent without point, ridiculous characters, bad acting, bad script and plain silly.\"',\n",
       " 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_docs(path='./movie_data.csv').next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get_minibatch関数を定義する。この関数は、stream_docs関数からドキュメントストリームを受け取り、size引数によって指定された個数のドキュメントを返す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream,size):\n",
    "    docs, y = [],[]\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- アウトオブコア学習にCountVectorizerは使用できない。アウトオブコア学習では、語彙が完全にメモリに読み込まれていることが要求される。\n",
    "- TfidfVectorizerは、逆文書頻度を計算するにあたって、トレーニングデータセットの特徴ベクトルがすべてメモリに読み込まれていることを要求する。\n",
    "- scikit-learnには、HashingVectoriaerという便利なベクトライザが実装されている。こいつはデータに依存しない。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error ='ignore',\n",
    "                        n_features=2**21, #特徴量が^21\n",
    "                        preprocessor=None,\n",
    "                        tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log', #ロジスティック回帰分類器で初期化\n",
    "                    random_state=1,\n",
    "                    n_iter=1)\n",
    "doc_stream = stream_docs(path='./movie_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- HashingVectorizerで特徴量の個数に大きな値を設定すると、ハッシュの衝突が発生する可能性は少なくなるが、ロジスティック回帰モデルの係数の値が大きくなることに注意。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0%                          100%\n",
      "[##############################] | ETA: 00:00:00\n",
      "Total time elapsed: 00:01:22\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0,1])\n",
    "for _ in range(45):\n",
    "    X_train,y_train = get_minibatch(doc_stream,size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train,y_train,classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forループでドキュメントの45個のミニバッチを処理している。ミニバッチはそれぞれ1，000個のドキュメントを構成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#最後の5，000個のドキュメントを使ってモデルの性能を評価する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.874\n"
     ]
    }
   ],
   "source": [
    "X_test,y_test = get_minibatch(doc_stream,size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f') % clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- アウトオブコア学習はメモリ効率が非常によく、完了が早い。\n",
    "- 最後の5，000個のドキュメントを使ってモデルを更新できる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
